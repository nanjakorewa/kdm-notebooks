{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57e4542b",
   "metadata": {},
   "source": [
    "# deeplで論文を翻訳する\n",
    "\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/_5j8KMIb0qc\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "\n",
    "## arXivから論文を取得する\n",
    "以下の論文をarXivからダウンロードして、概要を翻訳します。\n",
    "断りがない限り、コード中に出現する英文は以下の論文の「Abstract」の英文の一部です。\n",
    "\n",
    "Vaswani, Ashish, et al. \"[Attention is all you need.](https://arxiv.org/pdf/1706.03762.pdf)\" Advances in neural information processing systems 30 (2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faf601ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "論文タイトル：Attention Is All You Need\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "\n",
    "search = arxiv.Search(id_list=[\"1706.03762\"])\n",
    "paper = next(search.results())\n",
    "print(f\"論文タイトル：{paper.title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32dfa27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdf保存先：./1706.03762v5.Attention_Is_All_You_Need.pdf\n"
     ]
    }
   ],
   "source": [
    "pdf_path = paper.download_pdf()\n",
    "print(f\"pdf保存先：{pdf_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc2b86c",
   "metadata": {},
   "source": [
    "## pdfからテキストを抽出する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f8ccc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "アブスト開始位置：394, イントロ開始位置：1528\n",
      "The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks that include an encoder and a decoder. The bestperforming models also connect the encoder and decoder through an attentionmechanism. We propose a new simple network architecture, the Transformer,based solely on attention mechanisms, dispensing with recurrence and convolutionsentirely. Experimen...\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "\n",
    "abstract_text = \"\"\n",
    "with fitz.open(pdf_path) as pages:\n",
    "    first_page = pages[0]\n",
    "    text = first_page.get_text().replace(\"\\n\", \"\")\n",
    "    print(f'アブスト開始位置：{text.find(\"Abstract\")}, イントロ開始位置：{text.find(\"Introduction\")}')\n",
    "    abstract_text = text[text.find(\"Abstract\") + 8 : text.find(\"Introduction\") - 1]\n",
    "\n",
    "print(f\"{abstract_text[:400]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71fe13b",
   "metadata": {},
   "source": [
    "## deepl-pythonを使って英語を翻訳する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7506cfc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "おはようございます。\n"
     ]
    }
   ],
   "source": [
    "import deepl\n",
    "import os\n",
    "\n",
    "translator = deepl.Translator(os.getenv(\"DEEPL_AUTH_KEY\"))\n",
    "result = translator.translate_text(\"Good morning!\", source_lang=\"EN\", target_lang=\"JA\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95e2f1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "優性配列変換モデルは、エンコーダとデコーダを含む複雑なリカレントニューラルネットワークまたは畳み込みニューラルネットワークをベースにしています。また、最も優れたモデルでは、エンコーダとデコーダをアテンションメカニズムで接続しています。本研究では、再帰や畳み込みを必要とせず、注目メカニズムのみに基づいた新しいシンプルなネットワークアーキテクチャ「Transformer」を提案する。2つの機械翻訳タスクを用いた実験では、これらのモデルが優れた品質を持ち、並列化が可能で学習時間が大幅に短縮されることが示された。WMT 2014の英独翻訳タスクにおいて、我々のモデルは28.4 BLEUを達成し、アセンブルを含む既存の最良の結果よりも2 BLEU以上向上した。WMT 2014の英仏翻訳タスクでは、8つのGPUを用いて3.5日間の学習を行った結果、41.8という最新のBLEUスコアを達成しましたが、これは文献に掲載されている最高のモデルの学習コストのごく一部です。また、大規模および限定的な学習データを用いた英語の構文解析に適用することで、Transformerが他のタスクにもよく適応することを示しました。\n"
     ]
    }
   ],
   "source": [
    "result = translator.translate_text(abstract_text, source_lang=\"EN\", target_lang=\"JA\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
